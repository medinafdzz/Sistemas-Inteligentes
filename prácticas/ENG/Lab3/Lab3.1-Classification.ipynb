{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab 3.1: Machine Learning (Classification)**\n",
    "\n",
    "<hr>\n",
    "\n",
    "## **1. Introduction**\n",
    "Once we have analyzed, cleaned, and visualized our dataset, we can move on to the learning phase. Before we begin, it is necessary to identify the type of problem we are facing in order to select the most suitable methods or models.\n",
    "\n",
    "The following diagram illustrates the most common types of problems in the field of machine learning:\n",
    "\n",
    "<center><img src=\"ML_Diagram.png\" alt=\"diagram\" width=\"1000\"/></center>\n",
    "\n",
    "As you already know, problems that require **machine learning** to be solved are those where we do not know the *formula* that allows us to transform the input into the output. These problems are mainly divided into two types: **supervised** and **unsupervised**.\n",
    "\n",
    "In this course, we will focus on supervised learning problems, where we aim to predict either one or more classes (**classification**) or one or more numerical values (**regression**).\n",
    "\n",
    "Remember that in order to solve supervised learning problems, we always need **labeled data**, meaning data where we already know the expected output or correct label for a given input. These labeled examples will be used by the model to try to learn that *unknown formula* during training.\n",
    "\n",
    "We will begin by exploring **classification problems**, their characteristics, evaluation metrics, techniques, and how they are used to make predictions.\n",
    "\n",
    "### **Objective**\n",
    "In this practice, you will learn how to solve classification problems using different models and how to evaluate their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## **2. Problem Definition**\n",
    "\n",
    "To begin, we will attempt to create a model capable of solving a **binary classification** problem.\n",
    "\n",
    "In this case, we need to **create a model that, given the time (in seconds) of the 3 sectors of an *Aston Martin* driver, predicts whether the time was set by *Alonso* or not (*Stroll*)**.\n",
    "\n",
    "We will reload our data and generate the necessary dataset to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_pickle('https://raw.githubusercontent.com/AIC-Uniovi/Sistemas-Inteligentes/refs/heads/main/datasets/f1_23_monaco.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Create the variable <code>data_aston</code> with a DataFrame that contains only the data of that team and the columns \"Sector1Time\", \"Sector2Time\", \"Sector3Time\", and \"Driver\". Transform the sector columns from timedelta to seconds using <code>.dt.total_seconds()</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will modify the dataset to transform it into a binary classification problem. As you may recall, in this type of problem, the model predicts **a single value** that indicates the probability between zero ($0\\%$) and one ($100\\%$) that the given input belongs to the **positive class**.\n",
    "\n",
    "In our case, the **positive class** will be *\"Alonso\"*. Therefore, if the model predicts a $1$, it indicates that the given sector times belong to Alonso with $100\\%$ probability.\n",
    "\n",
    "If the model's prediction is $0$ or less than $0.5$, it indicates that the lap was not set by Alonso and therefore belongs to Stroll."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Create the column <code>Class</code> within the DataFrame <code>data_aston</code> so that it equals zero whenever the driver is not Alonso and 1 otherwise.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## **3. Baseline**\n",
    "\n",
    "Once we have a labeled dataset, where each entry (Sector1Time, Sector2Time, and Sector3Time) is associated with an expected output (Class), we can proceed to create a model to solve the binary classification problem.\n",
    "\n",
    "As explained in theory class, there are simple models that can provide very good performance without the need for more complex solutions. These models are called *baselines* and serve as a reference or lower bound. If a baseline performs better than a much more complex model, something is going wrong.\n",
    "\n",
    "In classification problems, there are three main ones:\n",
    "\n",
    "* **Random:** Predicts a 0 or 1 randomly without considering the input variables.\n",
    "* **Zero-R:** Predicts the majority class, that is, it analyzes the \"Class\" column of the dataset and, if the most frequent class is 1, it always predicts 1 for any input. As you can see, it doesn't use the input variables at all.\n",
    "* **One-R:** This model selects **one** input variable, the one that offers the best classification possible by itself. It is designed for categorical input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <strong>NOTE:</strong> We will use <i>Random</i> and <i>Zero-R</i> as baselines for our problem. <i>One-R</i> is not possible, as our input variables are numeric.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Baseline**\n",
    "We will create a model that generates random predictions (0 or 1) regardless of the input variables.\n",
    "\n",
    "The implementation is very simple: we just need to generate a random list of zeros and ones. Then, we will count how many times the model has made the correct prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set a seed so that the same random values are generated every time\n",
    "seed = 2533\n",
    "random.seed(seed)\n",
    "\n",
    "# Create a list with as many values (0 or 1) as rows in our dataset\n",
    "random_values = [random.choice([0, 1]) for _ in range(len(data_aston))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> How many times has the model been correct (in percentage)? To calculate this, you need to count how many times the random model predicted a 1 and it was actually a 1, and how many times it predicted a 0 and it was actually a 0.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    The value you just obtained is one of the most popular <strong>classification</strong> <strong>metrics</strong>, called <strong>accuracy</strong>. It simply counts the number of correct predictions made by the model over the maximum possible.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Zero-R**\n",
    "Next, we will obtain the result for the *Zero-R* baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> What is the majority class of the dataset? Considering this, what will be the <strong>accuracy</strong> of <i>Zero-R</i>?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Model Evaluation**\n",
    "\n",
    "At this point, we **already have two models** capable of solving our binary classification problem: the **Random** model and the **Zero-R** model. Additionally, we know that these models have an accuracy on **resubstitution** of approximately $35\\%$ and $60\\%$, respectively.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <strong>Resubstitution:</strong> The process by which a model is <u>trained and evaluated using the same dataset</u>. This procedure can provide an initial estimate of model performance, though <strong>it does not reflect its generalization ability</strong>.\n",
    "</div>\n",
    "\n",
    "It is important to remember that when we create a machine learning model, we aim for it to not only perform well with known data but **to be able to make predictions on new, unseen data**.\n",
    "\n",
    "With this in mind, we can see that the evaluation we just conducted on our models is not entirely appropriate. What we've measured is how well they predict examples they already know, but not their ability to make predictions on future data, which is what really matters.\n",
    "\n",
    "### **Strategies**\n",
    "To address this issue, rather than evaluating the model on the same data it learned from, we will evaluate it on a part of the dataset that we will have previously separated (test set).\n",
    "\n",
    "This subset is usually created by randomly selecting a percentage (typically $20\\%$) of examples from the original dataset and will be used solely to evaluate the model's performance, not for training. By doing this, we *\"simulate\"* future unseen cases, and if the model has good accuracy on this set, we will know that it is genuinely good.\n",
    "\n",
    "This strategy is called **Simple Validation**, but there are many other strategies:\n",
    "\n",
    "- **Cross-validation**: Involves splitting the data into several subsets (*folds*), training the model on some of them, and evaluating it on the remaining ones, repeating the process several times. This helps provide a more stable estimate of model performance.\n",
    "- **Validation with validation set**: A third subset, distinct from the test set, is used to adjust the model's hyperparameters and prevent overfitting. This is very typical when using neural networks.\n",
    "\n",
    "To get a more realistic evaluation of our models' performance, we will apply **simple validation** and recheck their accuracy, but this time on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Split the <code>data_aston</code> dataset into training and test sets (80% and 20%) and store the subsets in <code>data_aston_train</code> and <code>data_aston_test</code>. Use the <code><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">train_test_split()</a></code> function from the <i>scikit-learn</i> library, which you need to install in your conda environment first. \n",
    "    <hr>\n",
    "    <strong>Set the function's seed so that it always performs the same split (random_state=2533)</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> What is the accuracy of the Random model now, over the test set? Remember to set the seed again.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> What is the accuracy of the Zero-R model now, over the test set? \n",
    "    <hr>\n",
    "    Remember that you need to recalculate the majority class using the new training set (this is the only data your model should know) and use it to evaluate the model on the test set.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have seen, the models have obtained values very close to $50\\%$ accuracy on the test set, which is far from ideal.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <strong>Note:</strong> The worst possible binary classification model will have an accuracy of 50%, not 0%. A model with 0% accuracy would be predicting everything correctly, but its predictions would need to be inverted.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## **5. Scikit-Learn**\n",
    "\n",
    "The `Scikit-learn` library we used earlier contains a wide range of models and tools that will be useful for solving machine learning problems.\n",
    "\n",
    "For example, the baselines we just implemented are already incorporated in the `DummyClassifier()` class.\n",
    "\n",
    "This class has the `strategy` parameter, which allows us to select the desired baseline and can take, among others, the following values:\n",
    "- **uniform**: Equivalent to our *Random* baseline.\n",
    "- **most_frequent**: Equivalent to *Zero-R* baseline.\n",
    "\n",
    "In the following code, we can see how to use this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Create the models\n",
    "baseline_random = DummyClassifier(strategy = 'uniform', random_state = seed)\n",
    "baseline_zeror = DummyClassifier(strategy = 'most_frequent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the models are created, the next step is to train them, and for that, we need to provide the training data using the `fit()` method, specifically the $X$ and $Y$.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <strong>Note:</strong> In supervised learning, we refer to X as the independent variables or <i>inputs</i> and Y as the dependent variables or <i>outputs</i>. \n",
    "</div>\n",
    "\n",
    "As you know, the goal of the model is to *learn* the relationship between $X$ and $Y$ in order to predict $Y$ from new, unknown $X$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Create the variables X and Y to train the models. Remember that we have split the dataset into two parts.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# Train the models\n",
    "baseline_random.fit(X, Y)\n",
    "baseline_zeror.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, we can make predictions to evaluate its performance with data that was not seen during training. In `scikit-learn`, this is done using the `predict()` function.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <strong>Note:</strong> This function only takes X (not Y) and returns the predicted Y values.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data_aston_test[['Sector1Time', 'Sector2Time', 'Sector3Time']]\n",
    "Y_test = data_aston_test[['Class']]\n",
    "\n",
    "# Make predictions\n",
    "pred_random = baseline_random.predict(X_test)\n",
    "pred_zeror = baseline_zeror.predict(X_test)\n",
    "\n",
    "# Print results\n",
    "print(pred_random)\n",
    "print(pred_zeror)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another advantage of this library is that it includes many metrics within the `metrics` package, so we can easily obtain the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.accuracy_score(Y_test, pred_random))\n",
    "print(metrics.accuracy_score(Y_test, pred_zeror))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Obtain the confusion matrices for the models using <a href=\"https://scikit-learn.org/stable/api/sklearn.metrics.html\">the method implemented in the library</a>. Also, obtain the <i>Precision, Recall, and F1</i> scores using the methods or formulas.\n",
    "</div>\n",
    "\n",
    "<div style=\"width:800px;background:white;padding:10px\">\n",
    "    <img src=\"https://i.imgur.com/7WwY9bZ.jpeg\" style=\"margin-bottom:10px\"> </img>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## **6. Other Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have seen in the theory class, in addition to the baselines we have used, there are many other different models to solve classification problems. Some of them can be:\n",
    "\n",
    "* **Logistic Regression:** A \"linear\" model that uses a logistic function (sigmoid) to predict the probability of belonging to a class.\n",
    "* **K-Nearest Neighbors:** A classifier that assigns an instance to the most frequent class among its k nearest neighbors.\n",
    "* **Decision Trees:** A classification model that creates a tree allowing decisions based on the data features (inputs).\n",
    "* **SVM:** A classification algorithm that seeks the optimal hyperplane to maximize the margin between classes.\n",
    "* **Neural Networks:** We will explore them in detail in the upcoming topics.\n",
    "\n",
    "Just like with the baselines, `scikit-learn` provides us with most of these models already implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <strong>Note:</strong> Before using other models, it is necessary to carry out a phase of <u>data preprocessing</u> that we had pending: <strong>normalization or standardization</strong> of the data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may remember, this phase ensures that all the <u>inputs</u> to our model (sector times in our case) are in the same range (normalization) or have the same mean and standard deviation (standardization) to facilitate learning.\n",
    "\n",
    "We haven't done this so far because the previous baselines did not use any input information; one predicted randomly, and the other predicted the most frequent class.\n",
    "\n",
    "In this case, we will perform standardization (normalization would also be valid). Let's first check the mean and standard deviation of the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and standard deviation of the inputs in our model\n",
    "X.describe().loc[[\"mean\", \"std\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\"><code>StandardScaler()</code></a> from <i>sklearn</i> to standardize the training and test X data. Store the new data in <code>X_std</code> and <code>X_test_std</code>. Remember that the test data is unknown to us, so it should not influence the calculation of the mean and standard deviation.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "# Your code here\n",
    "\n",
    "# Verify with: \n",
    "print(X_std.mean(axis = 0))\n",
    "print(X_std.std(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.1. Logistic Regression**\n",
    "\n",
    "We are now ready to train and evaluate new models <u>aiming to improve the baselines' metrics</u>. We will start with **Logistic Regression**.\n",
    "\n",
    "To use it, we simply need to create an object of the `LogisticRegression()` class, train it with `fit()`, and evaluate it with `predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create, train, and evaluate the model\n",
    "model_log = LogisticRegression()\n",
    "model_log.fit(X_std, Y.squeeze())  # The squeeze removes unnecessary dimensions\n",
    "pred_log = model_log.predict(X_test_std)\n",
    "print(metrics.accuracy_score(Y_test, pred_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this model achieves better results in terms of accuracy, but not by much.\n",
    "\n",
    "We can also visualize its **confusion matrix**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conf_mat_log = metrics.confusion_matrix(Y_test, pred_log)\n",
    "\n",
    "plt.figure(figsize=(2,2)); # Tamaño del plot\n",
    "sns.heatmap(conf_mat_log, annot=True, cbar=False)\n",
    "plt.ylabel(\"Real (y)\")\n",
    "plt.xlabel(\"Predicción (y_hat)\")\n",
    "plt.title(\"Matriz de confusión\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.2. K-Nearest Neighbors**\n",
    "\n",
    "The next model we can try is the `KNeighborsClassifier()`, which is used exactly the same way as the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create, train, and evaluate the model\n",
    "model_knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "model_knn.fit(X_std, Y.squeeze())\n",
    "pred_knn = model_knn.predict(X_test_std)\n",
    "print(metrics.accuracy_score(Y_test, pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results in terms of accuracy with this model are very close to 100%, making it the best among those evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.3. Decision Trees**\n",
    "\n",
    "Next, we will analyze the performance of a decision tree in solving this task. The class within the library is called `DecisionTreeClassifier()` and it has numerous hyperparameters, with one of the most notable being `max_depth`, which allows setting the maximum depth of the tree and thus controlling **overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create, train, and evaluate the model\n",
    "model_tree = DecisionTreeClassifier(random_state = seed, max_depth = 2)\n",
    "model_tree.fit(X_std, Y.squeeze())\n",
    "pred_tree = model_tree.predict(X_test_std)\n",
    "print(metrics.accuracy_score(Y_test, pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is similar to the previous model, but this one has the advantage of allowing us to see its internal workings, i.e., the tree it has created. To do this, we will need to install a new library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import pydotplus\n",
    "\n",
    "# Export the resulting tree to DOT format\n",
    "dot_data = export_graphviz(decision_tree = model_tree, feature_names = X.columns, class_names = ['Stroll', 'Alonso'], filled = True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.4. Support Vector Machines (SVM)**\n",
    "\n",
    "The last model we are going to try is the SVM, implemented in the `SVC()` class. We will start with a **linear kernel**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create an SVM with a linear kernel, train, and evaluate\n",
    "model_svm = SVC(kernel = 'linear')\n",
    "model_svm.fit(X_std, Y.squeeze())\n",
    "pred_svm = model_svm.predict(X_test_std)\n",
    "print(metrics.accuracy_score(Y_test, pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is not very high, so it's possible that the classes are not **linearly separable**.\n",
    "\n",
    "To improve performance, we will now try a **polynomial kernel** of degree 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an SVM with a polynomial kernel of degree 2 and independent term 1. Train and evaluate\n",
    "model_svm_p = SVC(kernel = 'poly', degree = 2, coef0 = 1)\n",
    "model_svm_p.fit(X_std, Y.squeeze())\n",
    "pred_svm_p = model_svm_p.predict(X_test_std)\n",
    "print(metrics.accuracy_score(Y_test, pred_svm_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the non-linearity of this model allows it to solve the problem with a precision identical to the best models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Complete the function <code>train_and_eval()</code> for the remaining models, including both versions of the SVM. It uses the function <code>train_and_eval_model()</code> implemented below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_model(model_name, model, X_std, Y, X_test_std, Y_test):\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_std, Y.squeeze())\n",
    "\n",
    "    # Predictions\n",
    "    Y_train_pred = model.predict(X_std)\n",
    "    Y_test_pred = model.predict(X_test_std)\n",
    "\n",
    "    # Calculate metrics for training data\n",
    "    tr_accuracy = metrics.accuracy_score(Y, Y_train_pred)\n",
    "    tr_precision = metrics.precision_score(Y, Y_train_pred, zero_division = 0)\n",
    "    tr_recall = metrics.recall_score(Y, Y_train_pred)\n",
    "    tr_f1 = metrics.f1_score(Y, Y_train_pred)\n",
    "    \n",
    "    # Calculate metrics for test data\n",
    "    tst_accuracy = metrics.accuracy_score(Y_test, Y_test_pred)\n",
    "    tst_precision = metrics.precision_score(Y_test, Y_test_pred, zero_division = 0)\n",
    "    tst_recall = metrics.recall_score(Y_test, Y_test_pred)\n",
    "    tst_f1 = metrics.f1_score(Y_test, Y_test_pred)\n",
    "    \n",
    "    return (model_name, tr_accuracy, tr_precision, tr_recall, tr_f1, tst_accuracy, tst_precision, tst_recall, tst_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(X_std, Y, X_test_std, Y_test):\n",
    "\n",
    "    # Create a list to store the results of each model\n",
    "    all_results = []\n",
    "    \n",
    "    # Random baseline\n",
    "    baseline_aleatorio = DummyClassifier(strategy = 'uniform', random_state = seed)\n",
    "    model_results = train_and_eval_model('Random', baseline_aleatorio, X_std, Y, X_test_std, Y_test)\n",
    "    all_results.append(model_results)\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "    # Print the resulting dataframe\n",
    "    multi_index = pd.MultiIndex.from_tuples([ ('Model', 'Name'), ('Train', 'Accuracy'), ('Train', 'Precision'), ('Train', 'Recall'), ('Train', 'F1'), ('Test', 'Accuracy'), ('Test', 'Precision'), ('Test', 'Recall'), ('Test', 'F1') ])    \n",
    "    all_results = pd.DataFrame(all_results, columns = multi_index)\n",
    "    display(all_results)\n",
    "\n",
    "train_and_eval(X_std, Y, X_test_std, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Which model do you think is the best? Why?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your response here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Create a model capable of predicting, based on the 3 sectors of a driver, whether the tire used for the lap is for wet conditions (\"INTERMEDIATE\" or \"WET\") or not. Perform all necessary preprocessing and try all the classification models we have seen so far using the previous function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
