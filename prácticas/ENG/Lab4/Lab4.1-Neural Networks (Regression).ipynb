{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab 4.1: Neural Networks (Regression)**\n",
    "\n",
    "<hr>\n",
    "\n",
    "## **1. Introduction**  \n",
    "In previous practices, we learned how to solve classification and regression problems using traditional machine learning methods or models. In this practice, we will see how to solve these types of problems using **Neural Networks**, which are among the most commonly used methods today.  \n",
    "\n",
    "These networks form the foundation of *Deep Learning* and have the capability of being highly versatile as they are composed of multiple neurons organized into layers. These neurons are also called *perceptrons*, which is why neural networks are also known as **Multilayer Perceptrons**.  \n",
    "\n",
    "Just like with other models, `scikit-learn` provides a couple of classes that facilitate the use of these networks:  \n",
    "\n",
    "* **MLPRegressor:** A multilayer perceptron designed to solve regression problems.  \n",
    "* **MLPClassifier:** A multilayer perceptron designed to solve classification problems.  \n",
    "\n",
    "For this type of model, we will not use these predefined classes since we are interested in understanding their architecture and internal functioning. For this reason, we will use a new library, [`tensorflow`](https://www.tensorflow.org/), which is one of the most widely used in Python for deep learning tasks, along with [`pytorch`](https://pytorch.org/).  \n",
    "\n",
    "### **Objectives**  \n",
    "In this practice, you will learn to:  \n",
    "* Create and train Neural Networks.  \n",
    "* Optimize their hyperparameters.  \n",
    "* Add nonlinear activation functions.  \n",
    "\n",
    "We will start by installing the library in our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of neural networks is that they can be trained on either a CPU or a GPU.  \n",
    "\n",
    "Later on, we will see how to train one of these networks on the GPUs of the lab computers to accelerate the training process. On your personal computer, you will likely only be able to run it on a CPU.  \n",
    "\n",
    "Next, we load our data once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "seed = 2533\n",
    "data = pd.read_pickle('https://raw.githubusercontent.com/AIC-Uniovi/Sistemas-Inteligentes/refs/heads/main/datasets/f1_23_monaco.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## **2. Regression Problems**  \n",
    "\n",
    "We will attempt to solve the same problem as in the previous practice, which is:  \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Create a model that, given the time in the first sector `<code>Sector1Time</code>, can predict the total lap time <code>LapTime</code>.</b>\n",
    "</div> \n",
    "\n",
    "The first step will be to create the necessary datasets to train a model.  \n",
    "\n",
    "### **2.1. Data Preprocessing**\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "    <b>Exercise:</b> Separate the X and Y from the dataframe <code>data_sector2lap</code>, split it into training and test sets (80/20) by setting a random seed, and finally <b>standardize</b> the X values.  \n",
    "    <hr>  \n",
    "    When X has only one column, you must use double brackets (<code>data[['column_name']]</code> instead of <code>data['column_name']</code>) for <code>StandardScaler()</code> to work correctly.  \n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_sector2lap = data[['LapTime', 'Sector1Time']].copy()\n",
    "data_sector2lap['LapTime'] = data_sector2lap['LapTime'].dt.total_seconds()\n",
    "data_sector2lap['Sector1Time'] = data_sector2lap['Sector1Time'].dt.total_seconds()\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2. Machine Learning**  \n",
    "\n",
    "With the data ready, we will train and evaluate the well-known machine learning models once again to compare them with our new system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "    <b>Exercise:</b> Train and evaluate the remaining models (<i>K-Nearest Neighbors</i>, <i>Decision Trees</i>, and <i>SVR</i>) using the following function.  \n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "def evaluate_model(Y_test, preds_test, model_name):\n",
    "    metrics = {\n",
    "        \"Métrica\": [\"MAE\", \"MSE\", \"R²\"],\n",
    "        \"TEST\": [mean_absolute_error(Y_test, preds_test), \n",
    "                 mean_squared_error(Y_test, preds_test), \n",
    "                 r2_score(Y_test, preds_test)]\n",
    "    }\n",
    "    df = pd.DataFrame(metrics)\n",
    "    print(f\"Resultados para {model_name}:\")\n",
    "    print(df.to_string(index=False))\n",
    "    print()\n",
    "\n",
    "# Baseline mean  \n",
    "baseline_mean = DummyRegressor(strategy = 'mean')  \n",
    "baseline_mean.fit(X_train, Y_train)  \n",
    "preds_test = baseline_mean.predict(X_test)  \n",
    "evaluate_model(Y_test, preds_test, 'Baseline')  \n",
    "\n",
    "# Linear Regression  \n",
    "model_linear = LinearRegression()  \n",
    "model_linear.fit(X_train, Y_train)  \n",
    "preds_test = model_linear.predict(X_test)  \n",
    "evaluate_model(Y_test, preds_test, 'Linear')  \n",
    "\n",
    "# Your code here  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results should look something like this:\n",
    "\n",
    "<center>\n",
    "\n",
    "| Model                  | MAE Test  | MSE Test  | R² Test  |\n",
    "|------------------------|-----------|-----------|----------|\n",
    "| *Baseline*             | 5.882     | 52.489    | -0.008   |\n",
    "| *Linear*               | 1.056     |  2.298    |  0.956   |\n",
    "| *KNN*                  | 0.820     |  1.801    |  0.965   |\n",
    "| *Decision Trees*       | 1.110     |  3.575    |  0.931   |\n",
    "| *SVR*                  | 0.735     |  1.738    |  0.967   |\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3. Neural Network**  \n",
    "\n",
    "Once we have the datasets and results from the traditional models, we can now create our first neural network from scratch.  \n",
    "\n",
    "The steps to create and train a neural network are as follows:  \n",
    "\n",
    "1) Create the model architecture.  \n",
    "2) Define the optimizer, loss function, and compile the model.  \n",
    "3) Train and evaluate the model.  \n",
    "\n",
    "#### **2.3.1. Create Architecture**  \n",
    "\n",
    "The first thing we need to define is its architecture, meaning the number of fully connected layers that make up the network.  \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">  \n",
    "    <strong>Note:</strong> As you know, the size of the input and output layers is determined by the problem at hand. In this case, we have 1 input and 1 output.  \n",
    "</div>  \n",
    "\n",
    "The simplest network in this case would be as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random  \n",
    "import numpy as np  \n",
    "import tensorflow as tf  \n",
    "\n",
    "from tensorflow.keras.models import Sequential  \n",
    "from tensorflow.keras.layers import Dense, Input  \n",
    "\n",
    "# Set the seeds for the libraries to ensure results are reproducible.  \n",
    "os.environ['PYTHONHASHSEED'] = str(seed)  \n",
    "random.seed(seed)  \n",
    "np.random.seed(seed)  \n",
    "tf.random.set_seed(seed)  \n",
    "\n",
    "# Define the layers of the model  \n",
    "model = Sequential()  \n",
    "model.add(Input(shape = (1,)))  \n",
    "model.add(Dense(1, name = 'output_layer'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code simply creates a `Sequential()` model, meaning a model where layers are added one after the other.  \n",
    "\n",
    "Next, we introduce the following layers into it:  \n",
    "\n",
    "* `Input()`: Input layer. It is not a layer in itself; it simply allows the model to know the size of the inputs.  \n",
    "* `Dense()`: Fully connected layer ($y=Wx+b$). We specify the size as a required parameter, and `name` is optional.  \n",
    "\n",
    "To view the final architecture of our model, we can execute the `summary()` method, which also provides information about the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that, as we mentioned earlier, since `Input()` is not actually a layer, it does not even appear in the summary.  \n",
    "\n",
    "Another relevant piece of information is the number of parameters or weights in the model, i.e., how many $W$ and $b$ it needs to learn during training to correctly transform the input into the output.  \n",
    "\n",
    "In this case, there are only 2: one $w$ and one $b$.  \n",
    "\n",
    "#### **2.3.2. Compile the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling a model in Keras is an essential step before training it. It serves to configure the learning process, specifying how the model will be optimized and how its performance will be evaluated. Essentially, it defines the following key elements:\n",
    "\n",
    "*   **Optimizer:** Defines the algorithm that will be used to adjust the model's weights during training, aiming to minimize the loss function. Common examples are `Adam`, `SGD`, `RMSprop`, among others. Each optimizer has its own hyperparameters (such as learning rate) that can be adjusted.\n",
    "\n",
    "*   **Loss Function:** Measures the difference between the model's predictions and the actual values (labels). The goal of training is to **minimize this loss**. The choice of loss function depends on the type of problem (classification, regression, etc.). Examples: `categorical_crossentropy` (for multiclass classification), `binary_crossentropy` (for binary or multilabel classification), `mean_squared_error` or `mean_absolute_error` (for regression).\n",
    "\n",
    "Next, we compile the model by specifying the `Adam` optimizer with a `learning_rate = 0.05` and a regression loss function (`mean_absolute_error`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "learning_rate = 0.001\n",
    "optim = Adam(learning_rate = learning_rate)\n",
    "model.compile(loss = 'mean_absolute_error', optimizer = optim )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.3.3. Train and Evaluate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the training and evaluation of the model. In `keras`, training is done through the [`.fit()`](https://keras.io/api/models/model_training_apis/) method, just like in `scikit-learn`.  \n",
    "\n",
    "This method also takes in the training data ($X$ and $Y$) and iteratively adjusts the model's weights (the $W$ and $b$) to minimize the loss function.\n",
    "\n",
    "Specifically, this method performs the following steps:\n",
    "\n",
    "1) **Batch Creation:** Divides the entire training dataset into batches (blocks of examples) and obtains the model's prediction for each batch. Neural networks are designed to work with very large datasets, and often it's not feasible to train with the entire dataset at once, as we did in previous machine learning methods.\n",
    "2) **Calculates the Loss:** Compares the model's predictions $\\hat{Y}$ with the actual labels $Y$ and calculates the loss defined during compilation, $MAE$ in this case.\n",
    "3) **Calculates Gradients:** Uses the backpropagation algorithm to compute the gradients of the loss function with respect to the model's weights. The gradients indicate the direction and magnitude of the change needed in the weights to reduce the loss.\n",
    "\n",
    "<center>\n",
    "    <div style=\"border-radius:5px; padding:10px; background:white; max-width:900px\">\n",
    "        <img src=\"https://i.imgur.com/1tscXrJ.png\">   \n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "4) **Updates Weights:** Adjusts the model's weights using the optimizer based on the calculated gradients. The optimizer determines how the weights should be updated efficiently to minimize the loss.\n",
    "5) **Repeats the Process:** Repeats the above steps for each of the training batches. Once all batches are processed, the entire process can be repeated for a set number of epochs.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <strong>Note:</strong> An epoch represents a full pass through the entire training dataset.\n",
    "</div>\n",
    "\n",
    "Next, we train the model using the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model. The verbose parameter in the train method allows configuring the amount of information displayed in the console during training.\n",
    "history = model.fit(X_train, Y_train, validation_split = 0.2, batch_size = 64, epochs = 200, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <strong>Note:</strong> Every time you run the previous block, <strong>training will continue</strong> from where it left off. To train the model from scratch, you would need to recreate and recompile it.\n",
    "</div>\n",
    "\n",
    "As you can see, there is an argument in the `fit` method that we haven't discussed: `validation_split`.\n",
    "\n",
    "This argument reserves a fraction of the training data to use it as a validation set; in other words, it separates 20% of the examples from the training set (in this case, since `validation_split=0.2`).\n",
    "\n",
    "So now, our dataset is split into three distinct blocks: Train, Validation, and Test. This approach is also known as **metavalidation**.\n",
    "\n",
    "##### **Metavalidation**\n",
    "\n",
    "Until now, to validate our model, we have used **simple validation** or hold-out, meaning we split it into: Train and Test.\n",
    "\n",
    "This approach is very valid for cases where the models we want to evaluate do not have **hyperparameters**, like linear regression.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <strong>Note:</strong> The validation set is used to adjust the model's hyperparameters. <hr> \n",
    "    Remember that a hyperparameter is any value we can set when creating a model, for example, the K in KNN.\n",
    "</div>\n",
    "\n",
    "Neural networks have multiple hyperparameters to adjust such as `learning rate`, `batch size`, `epochs`, `number of layers`, `number of neurons`, etc. Therefore, a validation set is necessary.\n",
    "\n",
    "Now, the training process changes slightly: Based on the training data, the model calculates the loss and updates the weights accordingly, then *calculates the loss on the validation set* **but does not update the weights with this data**.\n",
    "\n",
    "This allows:\n",
    "\n",
    "* **Monitor overfitting:** If performance on the training set keeps improving but performance on the validation set stagnates or worsens, this indicates that the model is starting to overfit the training data and losing its ability to generalize to new data.\n",
    "* **Adjust hyperparameters:** We adjust the values of hyperparameters to select the configuration that produces the best performance on the validation data.\n",
    "\n",
    "<hr>\n",
    "\n",
    "##### **Why don't we use Test directly to adjust the hyperparameters?**\n",
    "\n",
    "Imagine you want to bake the perfect cake to take to a baking contest.\n",
    "\n",
    "* **Training set (train):** These are all the tests you do at home with different recipes, temperatures, and times. Here, you try many combinations to learn how each change affects the outcome.\n",
    "\n",
    "* **Validation set (val):** Every time you bake a cake at home, you let your friends or family try it. They tell you if it's too dry, too sweet, if the texture is good... With that feedback, you adjust the recipe: more sugar? less time in the oven? This is the process of adjusting hyperparameters.\n",
    "\n",
    "* **Test set (test):** This is the contest jury. You've never let them taste any of your cakes. They will evaluate whether, beyond your adjustments, your recipe truly works.\n",
    "\n",
    "**Where's the trap?**\n",
    "\n",
    "If, before the contest, you have the jury taste several cakes and tell you what to change, when you go to the contest, you'll already know what they like. \n",
    "You won't be testing if your recipe is good in general, but rather you'll have made a cake tailored to them.\n",
    "\n",
    "<hr>\n",
    "\n",
    "In summary: \n",
    "* **Train:** The dataset used to train the model, adjusting its weights to minimize the loss function. \n",
    "* **Validation:** The dataset used to monitor overfitting and optimize the model's hyperparameters, **but not the weights**. \n",
    "* **Test:** The dataset used to evaluate the final performance of the model trained with the optimal hyperparameters and data that was not used previously (neither for training nor for hyperparameter adjustment).\n",
    "\n",
    "The `fit()` method returns an object that stores the entire history of the model's training across the epochs. In this case, we store the result in `history`.\n",
    "\n",
    "Now let's create a function to visualize the evolution of the training and validation losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def plot_loss_history(history):\n",
    "    # Extract history data\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history.get('val_loss', []) # Empty by default\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    # Construct the DataFrame\n",
    "    data = pd.DataFrame({ 'Epoch': epochs, 'Loss': loss, 'Type': 'Train' })\n",
    "    # If validation exist, use the data\n",
    "    if val_loss:\n",
    "        val_df = pd.DataFrame({ 'Epoch': epochs, 'Loss': val_loss, 'Type': 'Validation' })\n",
    "        data = pd.concat([data, val_df], ignore_index=True)\n",
    "    # Create the graph\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.lineplot(data=data, x=\"Epoch\", y=\"Loss\", hue=\"Type\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Evolución de la Pérdida durante el Entrenamiento\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    # Show legend only if needed\n",
    "    if val_loss: plt.legend(title=\"Conjunto\")\n",
    "    else: plt.legend().remove() # Avoid empty legends\n",
    "    plt.show()\n",
    "\n",
    "# Call the function (after training the model)\n",
    "plot_loss_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> For simplicity, let's combine the creation and compilation of the network into one function. Complete the following code and retrain the network from scratch.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_one(learning_rate):\n",
    "    # Create and compile the model\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the network from scratch\n",
    "model_1 = neural_network_one(learning_rate = 0.001)\n",
    "\n",
    "# Train\n",
    "# Your code here\n",
    "\n",
    "# Visualize training\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Hyperparameter Tuning**\n",
    "\n",
    "Once everything is combined into a single code block, we can move on to the hyperparameter tuning part.\n",
    "\n",
    "There are many things we can adjust (`batch size`, `epochs`, `learning rate`, ...) but we will focus on the `learning rate`.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Record the results of the last epoch of the previous model in the following table and perform the necessary experiments to complete the rest of the rows.\n",
    "</div>\n",
    "\n",
    "<center>\n",
    "\n",
    "| Model                    | MAE Train  | MAE Val |\n",
    "|---------------------------|------------|---------|\n",
    "| *Neural Network (lr=0.001)* |            |         |\n",
    "| *Neural Network (lr=0.05)*  |            |         |\n",
    "| *Neural Network (lr=0.1)*   |            |         |\n",
    "\n",
    "<center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Train the final model with the best <code>learning_rate</code> and evaluate on test using the <code>.predict()</code> method and the <code>evaluate_model()</code> function created previously.\n",
    "    <hr>\n",
    "    Add the results to the table.\n",
    "</div>\n",
    "\n",
    "<center>\n",
    "\n",
    "| Model                   | MAE Test  | MSE Test  | R² Test  |\n",
    "|-------------------------|-----------|-----------|----------|\n",
    "| *Baseline*              | 5.882     | 52.489    | -0.008   |\n",
    "| *Linear*                | 1.056     |  2.298    |  0.956   |\n",
    "| *KNN*                   | 0.820     |  1.801    |  0.965   |\n",
    "| *Decision Trees*        | 1.110     |  3.575    |  0.931   |\n",
    "| *SVR*                   | 0.735     |  1.738    |  0.967   |\n",
    "| *Neural Network Linear* |           |           |          |\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Adding Non-Linearity**\n",
    "\n",
    "We will make some modifications and create a different, slightly more complex model to see if we can improve performance.\n",
    "\n",
    "As you may recall, not all problems have a linear solution. A basic neural network like ours is limited to performing linear combinations of numbers (sums and multiplications), meaning it transforms the input into the output using the formula $y = Wx + b$.\n",
    "\n",
    "To introduce non-linearity, we need to incorporate **non-linear activation functions**. These functions are applied to the output of each layer, allowing the network to learn more complex relationships between inputs and outputs. Some common activation functions include `ReLU`, `sigmoid`, and `tanh`. By adding these functions, the network can approximate non-linear functions and solve more challenging problems.\n",
    "\n",
    "<center>\n",
    "    <div style=\"border-radius:5px; padding:10px; background:white; max-width:900px\">\n",
    "        <img src=\"https://i.imgur.com/e7kd5fs.png\">   \n",
    "    </div>\n",
    "</center>\n",
    "\n",
    "Our current model has only one layer, the output layer. If we were to apply any of these functions to this layer, not only would we fail to introduce non-linearity, but we would also significantly restrict the range of values the model can predict.\n",
    "\n",
    "For example, if we only add a `sigmoid` function to the final layer, we would have a linear model whose output will **always** be limited to the range between 0 and 1. This is not ideal for our regression problem.\n",
    "\n",
    "To achieve non-linearity and avoid this limitation—while allowing the model to predict in a range of values between $(-\\infty , \\infty)$—we will add a hidden (or intermediate) layer and apply activation in this layer. This way, the final layer can generate outputs without the restrictions imposed by the activation function while still achieving non-linearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Create, within the provided function, a new neural network, but this time with two layers, the hidden one of <b>size 10</b> and with an activation function of <code>tanh</code> to add non-linearity.\n",
    "    <br>\n",
    "    Check the documentation for the <a href=\"https://keras.io/api/layers/core_layers/dense/\"><code>Dense()</code></a> layer.\n",
    "    <hr>\n",
    "    Find the best <code>learning_rate</code> and evaluate on test with the best version. Fill in both tables.\n",
    "</div>\n",
    "\n",
    "<center>\n",
    "\n",
    "| Model                    | MAE Train  | MAE Val |\n",
    "|---------------------------|------------|---------|\n",
    "| *Neural Network (lr=0.001)* |            |         |\n",
    "| *Neural Network (lr=0.05)*  |            |         |\n",
    "| *Neural Network (lr=0.1)*   |            |         |\n",
    "\n",
    "<center>\n",
    "<br> \n",
    "<center>\n",
    "\n",
    "| Model                   | MAE Test  | MSE Test  | R² Test  |\n",
    "|--------------------------|-----------|-----------|----------|\n",
    "| *Baseline*               | 5.882     | 52.489    | -0.008   |\n",
    "| *Linear*                 | 1.056     |  2.298    |  0.956   |\n",
    "| *KNN*                    | 0.820     |  1.801    |  0.965   |\n",
    "| *Decision Trees*         | 1.110     |  3.575    |  0.931   |\n",
    "| *SVR*                    | 0.735     |  1.738    |  0.967   |\n",
    "| *Linear Neural Network*  |           |           |          |\n",
    "| *Non-Linear Neural Network* |        |           |          |\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_two(learning_rate):\n",
    "    # Create and compile the model\n",
    "    \n",
    "    # Your code here\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create the network from scratch\n",
    "model_2 = neural_network_two(learning_rate = 0.001)\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Which model would you choose?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4. Multiple Inputs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Exercise:</b> Create a pair of networks capable of predicting <i>LapTime</i> from <i>SpeedI1</i>, <i>SpeedI2</i>, <i>SpeedFL</i>, <i>SpeedST</i>, and <i>TyreLife</i>. Adjust the hyperparameters for each model.\n",
    "</div>\n",
    "\n",
    "<center>\n",
    "\n",
    "| Model                    | MAE Test  | MSE Test  | R² Test  |\n",
    "|--------------------------|-----------|-----------|----------|\n",
    "| *Baseline*               | 5.858     | 54.094    | -0.008   |\n",
    "| *Linear*                 | 0.932     | 1.678     | 0.969    |\n",
    "| *KNN*                    | 0.785     | 1.743     | 0.968    |\n",
    "| *Decision Trees*         | 0.899     | 1.841     | 0.966    |\n",
    "| *SVR*                    | 0.938     | 2.724     | 0.949    |\n",
    "| *Neural Network 1*       |           |           |          |\n",
    "| *Neural Network 2*       |           |           |          |\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lap_time = data[['LapTime', 'SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST', 'TyreLife']].copy()\n",
    "data_lap_time = data_lap_time.dropna()\n",
    "data_lap_time['LapTime'] = data_lap_time['LapTime'].dt.total_seconds()\n",
    "\n",
    "X = data_lap_time[['SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST', 'TyreLife']]\n",
    "Y = data_lap_time['LapTime']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = seed, test_size = .2)\n",
    "\n",
    "standardizer = StandardScaler()\n",
    "X_train = standardizer.fit_transform(X_train)\n",
    "X_test = standardizer.transform(X_test)\n",
    "\n",
    "# Baseline mean\n",
    "baseline_mean = DummyRegressor(strategy = 'mean')\n",
    "baseline_mean.fit(X_train, Y_train)\n",
    "preds_test = baseline_mean.predict(X_test)\n",
    "evaluate_model(Y_test, preds_test, 'Baseline')\n",
    "\n",
    "# Linear Regression\n",
    "model_linear = LinearRegression()\n",
    "model_linear.fit(X_train, Y_train)\n",
    "preds_test = model_linear.predict(X_test)\n",
    "evaluate_model(Y_test, preds_test, 'Linear')\n",
    "\n",
    "# KNN\n",
    "model_knn = KNeighborsRegressor()\n",
    "model_knn.fit(X_train, Y_train)\n",
    "preds_test = model_knn.predict(X_test)\n",
    "evaluate_model(Y_test, preds_test, 'KNN')\n",
    "\n",
    "# Decision Trees\n",
    "model_tree = DecisionTreeRegressor()\n",
    "model_tree.fit(X_train, Y_train)\n",
    "preds_test = model_tree.predict(X_test)\n",
    "evaluate_model(Y_test, preds_test, 'Decision Trees')\n",
    "\n",
    "# SVR\n",
    "model_svr = SVR()\n",
    "model_svr.fit(X_train, Y_train)\n",
    "preds_test = model_svr.predict(X_test)\n",
    "evaluate_model(Y_test, preds_test, 'SVR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSII",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
